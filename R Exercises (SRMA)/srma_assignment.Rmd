---
title: "Systematic Reviews and Meta Analyses"
author: " (your name here) " 
date: "4 October 2019"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Systematic Reviews and Meta Analyses (SRMA) Tools

For this exercise, we'll be introducing "metafor()" one of many R packages that is tailor made for meta-analysis production. Using the package is simple, but some of the assumptions behind the choices you make are less straightforward. To get you started, we'll be drawing from two main examples, including the website Meta-forProject.org and a paper by Polanin, Hennessy and Tanner-Smith (2017) titled *"A Review of Meta-Analysis Packages in R."*  
We also sugest reviewing a complimentary paper by Rubio-Aparicio et al. (2018) titled *"Guidelines for Reporting Systematic Reviews and Meta-analyses,"* which provies a great breakdown of the main steps you should consider when conducting meta-analyses. We encourage you to take deep dives into all three of these resources as you move forward with your meta-analysis work.  

Note that you'll need to update to R version: 3.5.1 or better for this exercise.  

**(as usual, dont forget to insert your name at the top)**  

# Part 0. Install package and run library 

First, start by installing the needed package:  

```{r}

## Ensure you are running the correct version of R (3.5.1 or better)
## If not, stop to download the latest version
sessionInfo()

## First install the metafor packages (commented out # to prevent run errors)
#  install.packages("metafor")

## Initiate the metafor package
library(metafor)
## For an overview and introduction to the package type: help("metafor").

## load other helper packages
library(tidyverse)
library(here)

```
\newpage



# Part 1. Explore your data

Normally it goes without saying that you already have a sense of the data you've collected from countless hours of searching, screening, and extracting point estimates and meta-data from your sample literature. In this case, we're skipping all that and going right into the analysis, so let's start by geting familiar with this sample dataset from Polanin and colleagues (2017).  

First, load in the data:


```{r}
  ## Upload the dataset into global environment: 
  dat <- read.csv(here("Data_TDV.csv"), header=T) 
  
  ## Describe data
  head(dat)
  
  # Note, under the environment tab to your right, you can also click on 
  # the spreadsheet icon to examine the data more closely.
  
```

The data are from a SRMA paper by De La Rue and colleagues (2017) titled *"A Meta-Analysis of School-Based Interventions Aimed to Prevent or Reduce Violence in Teen Dating Relationships."*  
.  
The dataset comes from Polanin et al. 2017, and includes 8 variables from articles extracted in the meta-analysis. These include:  
.  
**1) Study.ID** - an identifier for each study;  
**2) ES.ID** - another identifier for each point estimate (multiple per study);  
**3) DOP** - the date of publication;  
**4) g** - the point estimate itself;  
**5) var** - the variance of each point estimate;  
**6) Follow.Up** - whether the point estimate comes from a follow-up group;  
**7) Random** - whether the study used randomization as an identification strategy; and  
**8) Perc.Males** - the percent of the sample that was male for each point estimate.  







\newpage 

# Part 2. Data preparation  
In this case, we're only interested in the "follow-up" data (where *Follow.Up==1*). Prepare data for traditional meta-analysis by creating subsets of the dataset: Non Follow-up & Follow-up called *dat.0* and *dat.1*, respectively:


```{r}
  dat.0 <- subset(dat, Follow.Up=="0")  ## for the non-follow up, ==0
  dat.1 <- subset(dat, Follow.Up=="1")  ## for the follow up, ==1
  
  ## In this case, we have isolated ONLY the subset of the data that is dealing
  ## with follow-up information (as opposed to post-test measures where
  ## Follow.Up = 0) from within each study (i.e. Follow.Up=1). We are then
  ## selecting 4 effect sizes from study #6 to create these new data frames. 
  
  print(dat.1)
  print(dat.0)
```


\newpage 
## 2.1 Data preparation (cont'd)  
In the case of our current dataset, there is one study (study #6) that has multiple effect sizes extracted for two different samples of follow-up data. We could include all of these effect sizes in our analysis, but we've decided that it will place too much weight on each of the study samples, so we need to reduce that study into an average of the effect sizes in the two samples (you can also do this through weighting, but for now we'll do it manually).  
Because this R exercise is a tool for you, we've done this for you below.  
```{r}
  ## Create a smaller datasets with the samples from study 6 that 
   # include multiple effect sizes

  dat.1.117 <- dat.1[c(4,5),]
  dat.1.118 <- dat.1[c(6,7),]
  
  ## If you look closely at the last column for "% male" it gives you a hint
   # that we are picking effect sizes from two different samples within the 
   # same study - sample 117 has an perc.male = 45.03, while 118 has 
   # perc.male = 45.91.  
  
      ## Take a quick look at what you've extracted
      print(dat.1.117)
      print(dat.1.118)
  
## Now we must take the average of the effect sizes within each study sample
avg.g.117 <- mean(dat.1.117$g)      ## Here you're taking the average effect size (g)
avg.var.117 <- mean(dat.1.117$var)  ## Here, the average variance (var)
  
avg.g.118 <- mean(dat.1.118$g)   
avg.var.118 <- mean(dat.1.118$var)
  
## Replace the multiple effect sizes with average effect sizes
  dat.1.117[1,4] <- avg.g.117     ## Superimposes the average of g over the first row
  dat.1.117[1,5] <- avg.var.117   ## Superimposes the average of var over the first row
  dat.1.117 <- dat.1.117[-c(2),]  ## Removes the second row
  
  dat.1.118[1,4] <- avg.g.118
  dat.1.118[1,5] <- avg.var.118
  dat.1.118 <- dat.1.118[-c(2),]     
  
      print(dat.1.117)
      print(dat.1.118)
  
  # Finally, combine the rows with the new effect sizes and variances with the
  #  original dat.1 dataset (rbind(dat.1, dat.1.117, dat.1.118)) and remove 
  #  the old rows with the unused data (dat.1[-c(4,5,6,7,12,14),]). What 
  #  remains is a meta-analytic data set that has independent effect sizes and
  #  variances
  
  dat.1 <- rbind(dat.1, dat.1.117, dat.1.118)
  dat.1 <- dat.1[-c(4,5,6,7),]
  
      print(dat.1)
```


\newpage 

# Part 3. Meta analysis procedures  
## Run 1.
Now that you have your data in the shape you want, its time to start analyzing. First, get familiar with the metafor package by typing help("metafor").
Then, we'll start by running a simple random effects model.  

*example one*
```{r}

  # Run.1 : A simple meta-analysis with no moderators
      # The yi and vi are for the effect size and variance respectively
      # The data argument tells R which dataset to use
      # The method argument is reserved for the type of meta-analytic model to run
      # In this case, random-effects using maximum likelihood estimator (ML) 
  
  
  run.1 <- rma.uni(yi = g,         #effect size
                   vi = var,       #variance
                   data = dat.1,   #dataset
                   method = "ML")  #model estimation
  summary(run.1)

```
You'll be interested in the coefficient under "estimate". In this case, -.02642, significant with a p-value of 0.0212. You can also see estimates of the I^2, and test statistics for sample heterogeneity.

\newpage

**Question 1. Run a fixed effects model on the dat.1 data and call it "run.1.fe". Compare the output of run.1 to run.1.fe.  **
```{r}

```

**a. Why would you run a fixed effects versus a random effects model to conduct a meta-analysis?**



\newpage
## Run 2. Moderator analysis   
  Following Rubio-Aparicio et al. (2018), there are three potential categories of moderators:  
**1. Methodological**: Designs and methods used in the studies (e.g. random vs. non-random assignment) which may introduce differential risk of bias  
**2. Substantive**: Those related to the research question (e.g. socioeconomic characteristics, gender, etc.)  
**3. Extrinsic**: Those having nothing to do with the research enterprise, and should therefore not be related to the study results (e.g. publication status, educational background of main author, etc.)


*example two*
```{r}
    # A one-way analysis of variance using "mods" to examine moderator effects
    # The "-1" at the end of the statement tells R to remove the intercept
    # Therefore, the coefficients in the model are the meta-analytic average 
     # effect sizes for each level (i.e. random & non-random)
    # The "Test of Moderators" provides the Q-between statistic which indicates 
     # whether the difference between these two groups was statistically significant. 
  
  run.2 <- rma.uni(yi = g,
                   vi = var,
                   data = dat.1,
                   method = "ML",
                   mods =~ factor(Random) - 1) #moderator argument
```



\newpage
**Question 2. Use the summary function to compare run.1 to run.2.**
```{r}

```

**a. What does the Q-between statistic tell us about our chosen moderator?**  

**b. What is the effect of adding a moderator to the analysis (comparing run.1 to run.2)?**  



\newpage
## Runs 3 and 4. Subgroup Analysis  
Sometimes we want to examine different subgroups within our data. To do this, we can specify the "subset = " argument in the rma.uni() function.  

*example three*
```{r}
# Run 3 is a subgroup analysis isolating studies that do NOT use random
# assignment to those without. Herein, the data is split (or subset) 
# into the two groups that represent random and non-random assignment. 
  
  run.3 <- rma.uni(yi = g, 
                   vi = var,
                   data = dat.1,
                   method = "ML",
                   subset = (Random=="0"))  # Subgroup argument
  summary(run.3)
```




\newpage
**Question 3. Use the subset argument to create a run.4 that examines our main outcome when randomization was used. Compare this to run.3.**
```{r}

```

**a. Discuss the difference in effect sizes and significance.**  

**b. What does this tell us about studies that use random assignment?**


\newpage
# Part 4. Data visualization  
In this final part of the assignment we'll use the metafor() package to create three common data vizualizations used in meta analysis: L'abbe Plots, Forest Plots, and Funnel Plots.  


## 4.1 Funnel Plots  
First, lets examine funnel plots. You can read about them by typing help("funnel"). Funnel plots are a form of scatterplot of treatmente effects against a measure of study percision. These plots are used primarily as a visual aid for detecting bias in your meta-analysis. A symmetric inverted funnel shape arises from a "well-behaved" dataset where publication bias is less likely.  
In the example below, we'll use the data from our first run to examine how well-behaved our whole dataset was before playing with subsets. We can explore several measures including standard errors, sampling variance, and the inverse of both.

*example four*
```{r}
## Create space for the four plots in one image 
par(mfrow=c(2,2))
  
  ### draw funnel plots (using run.1)
  ###################################
  funnel(run.1, main="Standard Error")
  funnel(run.1, yaxis="vi", main="Sampling Variance")
  funnel(run.1, yaxis="seinv", main="Inverse Standard Error")
  funnel(run.1, yaxis="vinv", main="Inverse Sampling Variance")
```


\newpage
**Question 4. Create a Funnel plot for the standard errors of run.4**
```{r}

```
**a. Describe the funnel plot, and explain what it displays in the case of run.4.**  
*(hint, review Jack's SRMA lecture for a refresher)*


\newpage
## 4.2 Forest Plots  
Finally, we examine the use of forest plots. There is a tremendous amount of ancillary material in the R helpfiles, so check out help("forest") if you'd like to make these plots very pretty.  
In the meantime, lets take the data from the regression in run.1, and create a forest plot of the results.


*example five*
```{r}

## Redace space for one plot in one image 
par(mfrow=c(1,1))

forest(run.1)
 ## This is the simplest possible option for running a forest plot. 
  # You can add lots of bells and whistles including additional titles, 
  # headings, formatting rules, and the like.  

```
Here we've plotted each of the effect sizes by study, with the accompanying confidence intervals. If the confidence interval crosses over the zero dotted line, that tells us the point estimate was insignificant. If the CI does not cross zero, the effects are significant. The diamond shape at the bottom that corresponds to the **"RE Model"** heading is the combined effect of all the point estimates as given by the random effects model. 


\newpage
**Question 5. Compare a forest plot of run.1 to the forest plot from run.4**
```{r}

```
**a. Describe the difference between the two plots. **

